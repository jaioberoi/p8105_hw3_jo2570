---
title: "hw3_jo2570"
author: "Jai Oberoi"
date: "09/10/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading in tidyverse 
```{r}
library(tidyverse)
```

Problem 1

Loading in Instacart data from the p8105 datasets: 
```{r}
library(p8105.datasets)
data("instacart")
```

Describing the Instacart data: 

the size of the `instacart` dataset is `r nrow(instacart)` rows and `r ncol(instacart)` columns.
we can see that the average order hour of the day is `r mean(pull(instacart, order_hour_of_day))` and the median hour is `r mean(pull(instacart, order_hour_of_day))`. The average number of days since a prior order for a user/customer is `r mean(pull(instacart, days_since_prior_order))` and the median value is `r median(pull(instacart, days_since_prior_order))`. 

We can also get a breif run-down on the data by looking the results of this skim code:
```{r}
skimr::skim(instacart)
```

```{r}
instacart %>%
  count(aisle_id) %>%
  arrange(desc(n))
```
Aisles 83 and 24 have the most products ordered with a count of over 150,000

Creating a plot showing the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered: 
```{r}
instacart %>%
  count(aisle_id) %>%
  filter(n > 10000) %>%
  ggplot(aes(x = aisle_id, y = n)) + geom_line() + 
  labs( title = "Number of Items Ordered for Aisles with n > 10000", x = "Aisle", y = "Number of Items Ordered")
```

Creating a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”: 
```{r}
instacart %>%
     filter(aisle %in% c("baking ingredients", 
            "dog food care", 
            "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>% 
  summarize(n = n()) %>%
  top_n(3) %>%
  arrange(desc(n)) %>%
  knitr::kable()
```

Creating a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week: 
```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_order_hour_of_day = mean(order_hour_of_day)) %>%
  select(product_name, order_dow, mean_order_hour_of_day) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_order_hour_of_day
  ) %>%
  knitr::kable()
```

Problem 2 

Loading in the BRFSS dataset:
```{r}
data("brfss_smart2010")
```

Cleaning the BRFSS dataset:
```{r}
brfss_clean = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic %in% c("Overall Health"),
         response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>%
  mutate(response = factor(response, labels = c("Poor","Fair","Good","Very good", "Excellent"))) 
```

To find out which states were observed at 7 or more locations in 2002:
```{r}
brfss_clean %>% 
  filter(year == "2002") %>% 
  group_by(locationabbr) %>%
  summarize(
    n = n_distinct(geo_location)) %>%
  filter(n > 6) %>% 
  arrange(desc(n))
```
PA, MA, NJ, CT, FL, NC


To find out which states were observed at 7 or more locations in 2010:
```{r}
brfss_clean %>% 
  filter(year == "2010") %>% 
  group_by(locationabbr) %>%
  summarize(
    n = n_distinct(geo_location)) %>%
  filter(n > 6) %>% 
  arrange(desc(n)) 
```
FL, NJ, TX, CA, MD, NC, NE, WA, MA, NY, OH, CO, PA, SC


Limiting the cleaned dataset to only "Excellent" responses: 
```{r}
brfss_excellent = brfss_clean %>% 
  filter(response == "Excellent") %>% 
  group_by(locationabbr, year, response) %>%
  summarise(mean_data_value = mean(data_value))
```

Creating a “spaghetti” plot of this average value over time within a state (from brfss_excellent): 
```{r}
ggplot(brfss_excellent, aes(x = year, y = mean_data_value, color = locationabbr)) + 
  geom_line(se = FALSE, na.rm = TRUE)
```

Creating a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State: 
```{r}
brfss_ny = brfss_clean %>%
  filter(topic %in% c("Overall Health"), 
         year %in% c("2006", "2010"),
         response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"),
         locationabbr %in% c("NY"))

brfss_ny %>% 
  ggplot(aes(x = response, y = data_value, color = response)) + geom_boxplot() + facet_grid(. ~year)
```


Problem 3 

Reading in the accelerometer data:
```{r}
accel_data = read_csv(file = "./data/accel_data.csv") %>% 
  janitor::clean_names()
```

Cleaning / wrangling accel_data: 
```{r}
accel_clean = accel_data %>% 
  mutate(
    day = str_to_lower(day),
    day = factor(day, labels = c("monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday")), 
    weekend = day %in% c("saturday", "sunday"),
    weekday = day %in% c("monday", "tuesday", "wednesday", "thursday", "friday"),
    weekday_weekend = ifelse(weekend, 1, 0)
  ) %>% 
  select(-weekend, -weekday)
```

Aggregating activities into daily_ativities and creating a plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week: 
```{r}
accel_daily = accel_clean %>% 
  pivot_longer(activity_1:activity_1440,
               names_to = "activity_minute",
               values_to = "activity") %>% 
    group_by(day_id, day, weekday_weekend) %>% 
  summarise(daily_activity = sum(activity))

accel_daily %>% 
  ggplot(aes(x = day_id, y = daily_activity, color = day)) + geom_point() + geom_line()

#There are no apparent trends (weednesdays and some weekdats have lower activity)
```





